{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML 3204 Term Project\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team\n",
    "* Ankur Kishorbhai Rokad (C0793757)\n",
    "* Avinash Ravi\n",
    "* Vishal Sabarinath Venkatesan\n",
    "* Sahista Patel (C0796681)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 1: Fetching & Sentiment Analysis\n",
    "*This file has tweet fetching and sentiment analysis process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.255643Z",
     "start_time": "2021-11-23T20:13:25.185595Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords as st\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T14:24:58.363586Z",
     "start_time": "2021-03-01T14:24:58.355342Z"
    }
   },
   "source": [
    "## Create Tweepy instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.271377Z",
     "start_time": "2021-11-23T20:13:27.257578Z"
    }
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('sspVRoqh0CNUHxpipX5nwAX3H', 'fBlMPlGQGFCVONWSavlIPgAb4XQY9uLWvRZdmSA945NLP35UlE')\n",
    "auth.set_access_token('866910653495164928-LnEAwUZFU2gZf78o2M0nzEz32yLpma5', 'w4Yx880veGohVYxOHeAp3l2DHF7b1dI0HAMU0hm8J5DUo')\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.287127Z",
     "start_time": "2021-11-23T20:13:27.274876Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk_stopwords = st.words('english')\n",
    "stopwords = set(nltk_stopwords)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.317718Z",
     "start_time": "2021-11-23T20:13:27.289123Z"
    }
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the tweets into desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.333811Z",
     "start_time": "2021-11-23T20:13:27.320604Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tweets(tweets, movie_name, movie_id):\n",
    "    processed_tweets = []\n",
    "    for tweet in tweets:       \n",
    "        text = clean_text(tweet['full_text'])\n",
    "        tweet['created_at'] = format_date_time(tweet['created_at'])\n",
    "        processed_tweets.append( {\n",
    "            'movie_id' :movie_id,\n",
    "            'movie_name' :movie_name,\n",
    "            'created_at':tweet['created_at'],\n",
    "            'tweeter_id': tweet['id_str'],\n",
    "            'text': text,\n",
    "            'raw_text' : tweet['full_text'],\n",
    "            'created_day' : datetime.strftime(datetime.strptime(tweet['created_at'],'%Y-%m-%d %H:%M:%S'),'%Y-%m-%d'),\n",
    "            'sent_score' : analyzer.polarity_scores(text)\n",
    "        })\n",
    "        \n",
    "    return processed_tweets\n",
    "\n",
    "def get_json_from_response(response):\n",
    "    return list(map(lambda a: a._json, response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a data-frame from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.349825Z",
     "start_time": "2021-11-23T20:13:27.335808Z"
    }
   },
   "outputs": [],
   "source": [
    "def dict_to_df(tweets):\n",
    "    df_dict = {\n",
    "        'movie_id' : list(map(lambda a: a['movie_id'], tweets)),\n",
    "        'movie_name' : list(map(lambda a: a['movie_name'], tweets)),\n",
    "        'created_at' : list(map(lambda a: a['created_at'], tweets)),\n",
    "        'tweeter_id' : list(map(lambda a: a['tweeter_id'], tweets)),\n",
    "        'text' : list(map(lambda a: a['text'], tweets)),   \n",
    "        'raw_text' : list(map(lambda a: a['raw_text'], tweets)),     \n",
    "        'created_day' : list(map(lambda a: a['created_day'], tweets)),\n",
    "        'sent_score' : list(map(lambda a: a['sent_score'], tweets))\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(df_dict) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.396490Z",
     "start_time": "2021-11-23T20:13:27.351788Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = re.sub(\"http\\S+\", '', str(text))\n",
    "\n",
    "    # Contractions ref: https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n",
    "    text = re.sub(r\"he's\", \"he is\", str(text))\n",
    "    text = re.sub(r\"there's\", \"there is\", str(text))\n",
    "    text = re.sub(r\"We're\", \"We are\", str(text))\n",
    "    text = re.sub(r\"That's\", \"That is\", str(text))\n",
    "    text = re.sub(r\"won't\", \"will not\", str(text))\n",
    "    text = re.sub(r\"they're\", \"they are\", str(text))\n",
    "    text = re.sub(r\"Can't\", \"Cannot\", str(text))\n",
    "    text = re.sub(r\"wasn't\", \"was not\", str(text))\n",
    "    text = re.sub(r\"aren't\", \"are not\", str(text))\n",
    "    text = re.sub(r\"isn't\", \"is not\", str(text))\n",
    "    text = re.sub(r\"What's\", \"What is\", str(text))\n",
    "    text = re.sub(r\"haven't\", \"have not\", str(text))\n",
    "    text = re.sub(r\"hasn't\", \"has not\", str(text))\n",
    "    text = re.sub(r\"There's\", \"There is\", str(text))\n",
    "    text = re.sub(r\"He's\", \"He is\", str(text))\n",
    "    text = re.sub(r\"It's\", \"It is\", str(text))\n",
    "    text = re.sub(r\"You're\", \"You are\", str(text))\n",
    "    text = re.sub(r\"I'M\", \"I am\", str(text))\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", str(text))\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", str(text))\n",
    "    text = re.sub(r\"i'm\", \"I am\", str(text))\n",
    "    text = re.sub(r\"I'm\", \"I am\", str(text))\n",
    "    text = re.sub(r\"Isn't\", \"is not\", str(text))\n",
    "    text = re.sub(r\"Here's\", \"Here is\", str(text))\n",
    "    text = re.sub(r\"you've\", \"you have\", str(text))\n",
    "    text = re.sub(r\"we're\", \"we are\", str(text))\n",
    "    text = re.sub(r\"what's\", \"what is\", str(text))\n",
    "    text = re.sub(r\"couldn't\", \"could not\", str(text))\n",
    "    text = re.sub(r\"we've\", \"we have\", str(text))\n",
    "    text = re.sub(r\"who's\", \"who is\", str(text))\n",
    "    text = re.sub(r\"y'all\", \"you all\", str(text))\n",
    "    text = re.sub(r\"would've\", \"would have\", str(text))\n",
    "    text = re.sub(r\"it'll\", \"it will\", str(text))\n",
    "    text = re.sub(r\"we'll\", \"we will\", str(text))\n",
    "    text = re.sub(r\"We've\", \"We have\", str(text))\n",
    "    text = re.sub(r\"he'll\", \"he will\", str(text))\n",
    "    text = re.sub(r\"Y'all\", \"You all\", str(text))\n",
    "    text = re.sub(r\"Weren't\", \"Were not\", str(text))\n",
    "    text = re.sub(r\"Didn't\", \"Did not\", str(text))\n",
    "    text = re.sub(r\"they'll\", \"they will\", str(text))\n",
    "    text = re.sub(r\"they'd\", \"they would\", str(text))\n",
    "    text = re.sub(r\"DON'T\", \"DO NOT\", str(text))\n",
    "    text = re.sub(r\"they've\", \"they have\", str(text))\n",
    "    text = re.sub(r\"i'd\", \"I would\", str(text))\n",
    "    text = re.sub(r\"should've\", \"should have\", str(text))\n",
    "    text = re.sub(r\"where's\", \"where is\", str(text))\n",
    "    text = re.sub(r\"we'd\", \"we would\", str(text))\n",
    "    text = re.sub(r\"i'll\", \"I will\", str(text))\n",
    "    text = re.sub(r\"weren't\", \"were not\", str(text))\n",
    "    text = re.sub(r\"They're\", \"They are\", str(text))\n",
    "    text = re.sub(r\"let's\", \"let us\", str(text))\n",
    "    text = re.sub(r\"it's\", \"it is\", str(text))\n",
    "    text = re.sub(r\"can't\", \"cannot\", str(text))\n",
    "    text = re.sub(r\"don't\", \"do not\", str(text))\n",
    "    text = re.sub(r\"you're\", \"you are\", str(text))\n",
    "    text = re.sub(r\"i've\", \"I have\", str(text))\n",
    "    text = re.sub(r\"that's\", \"that is\", str(text))\n",
    "    text = re.sub(r\"i'll\", \"I will\", str(text))\n",
    "    text = re.sub(r\"doesn't\", \"does not\", str(text))\n",
    "    text = re.sub(r\"i'd\", \"I would\", str(text))\n",
    "    text = re.sub(r\"didn't\", \"did not\", str(text))\n",
    "    text = re.sub(r\"ain't\", \"am not\", str(text))\n",
    "    text = re.sub(r\"you'll\", \"you will\", str(text))\n",
    "    text = re.sub(r\"I've\", \"I have\", str(text))\n",
    "    text = re.sub(r\"Don't\", \"do not\", str(text))\n",
    "    text = re.sub(r\"I'll\", \"I will\", str(text))\n",
    "    text = re.sub(r\"I'd\", \"I would\", str(text))\n",
    "    text = re.sub(r\"Let's\", \"Let us\", str(text))\n",
    "    text = re.sub(r\"you'd\", \"You would\", str(text))\n",
    "    text = re.sub(r\"It's\", \"It is\", str(text))\n",
    "    text = re.sub(r\"Ain't\", \"am not\", str(text))\n",
    "    text = re.sub(r\"Haven't\", \"Have not\", str(text))\n",
    "    text = re.sub(r\"Could've\", \"Could have\", str(text))\n",
    "    text = re.sub(r\"youve\", \"you have\", str(text))\n",
    "\n",
    "    # Abbreviations\n",
    "    text = re.sub(\"U.S.\", \"United States\", str(text))\n",
    "    text = re.sub(\"Dec\", \"December\", str(text))\n",
    "    text = re.sub(\"Jan.\", \"January\", str(text))\n",
    "\n",
    "    # Typos, slang and informal abbreviations\n",
    "    text = re.sub(r\"w/e\", \"whatever\", str(text))\n",
    "    text = re.sub(r\"w/\", \"with\", str(text))\n",
    "    text = re.sub(r\"USAgov\", \"USA government\", str(text))\n",
    "    text = re.sub(r\"recentlu\", \"recently\", str(text))\n",
    "    text = re.sub(r\"Ph0tos\", \"Photos\", str(text))\n",
    "    text = re.sub(r\"amirite\", \"am I right\", str(text))\n",
    "    text = re.sub(r\"exp0sed\", \"exposed\", str(text))\n",
    "    text = re.sub(r\"<3\", \"love\", str(text))\n",
    "    text = re.sub(r\"amageddon\", \"armageddon\", str(text))\n",
    "    text = re.sub(r\"Trfc\", \"Traffic\", str(text))\n",
    "    text = re.sub(r\"WindStorm\", \"Wind Storm\", str(text))\n",
    "    text = re.sub(r\"TRAUMATISED\", \"traumatized\", str(text))\n",
    "\n",
    "    # Character entity references\n",
    "    text = re.sub(r\"&gt;\", \">\", str(text))\n",
    "    text = re.sub(r\"&lt;\", \"<\", str(text))\n",
    "    text = re.sub(r\"&amp;\", \"&\", str(text))\n",
    "\n",
    "    # Remove Urls\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url.sub(r'', text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'', text)\n",
    "\n",
    "    # Remove emoticons, symbols, pictoraphs, transport and map symbols & flags\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Punctuations & special characters\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", str(text))\n",
    "\n",
    "    # Stop word removal\n",
    "    text = [str(i).lower() for i in text.split() if i.lower() not in stopwords]\n",
    "\n",
    "    # Stemming the words\n",
    "    text = [ps.stem(word) for word in text]\n",
    "\n",
    "    # Lemmatizing the words\n",
    "    text = [lm.lemmatize(word) for word in text]\n",
    "\n",
    "    # Joining back to a sentence\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formating the date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.412561Z",
     "start_time": "2021-11-23T20:13:27.401837Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_date_time(time_str):\n",
    "    return datetime.strftime(datetime.strptime(time_str,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.427998Z",
     "start_time": "2021-11-23T20:13:27.416190Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_tweets(query, count=300):\n",
    "    return api.search_tweets(q=query,\n",
    "                             count=count,\n",
    "                             include_rts=False,\n",
    "                             tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.443657Z",
     "start_time": "2021-11-23T20:13:27.430736Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_file_name(tweeter_id):\n",
    "    return tweeter_id + \"_\" + (''.join(\n",
    "        random.choice(string.ascii_lowercase) for i in range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T20:13:27.536671Z",
     "start_time": "2021-11-23T20:13:27.446644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>video_release_date</th>\n",
       "      <th>IMDb URL</th>\n",
       "      <th>unknown</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children's</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Get%20Shorty%...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Copycat%20(1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id              title release_date  video_release_date  \\\n",
       "0        1   Toy Story (1995)  01-Jan-1995                 NaN   \n",
       "1        2   GoldenEye (1995)  01-Jan-1995                 NaN   \n",
       "2        3  Four Rooms (1995)  01-Jan-1995                 NaN   \n",
       "3        4  Get Shorty (1995)  01-Jan-1995                 NaN   \n",
       "4        5     Copycat (1995)  01-Jan-1995                 NaN   \n",
       "\n",
       "                                            IMDb URL  unknown  Action  \\\n",
       "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...        0       0   \n",
       "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...        0       1   \n",
       "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...        0       0   \n",
       "3  http://us.imdb.com/M/title-exact?Get%20Shorty%...        0       1   \n",
       "4  http://us.imdb.com/M/title-exact?Copycat%20(1995)        0       0   \n",
       "\n",
       "   Adventure  Animation  Children's  ...  Fantasy  Film-Noir  Horror  Musical  \\\n",
       "0          0          1           1  ...        0          0       0        0   \n",
       "1          1          0           0  ...        0          0       0        0   \n",
       "2          0          0           0  ...        0          0       0        0   \n",
       "3          0          0           0  ...        0          0       0        0   \n",
       "4          0          0           0  ...        0          0       0        0   \n",
       "\n",
       "   Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
       "0        0        0       0         0    0        0  \n",
       "1        0        0       0         1    0        0  \n",
       "2        0        0       0         1    0        0  \n",
       "3        0        0       0         0    0        0  \n",
       "4        0        0       0         1    0        0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie = pd.read_csv(\n",
    "    \"data/u.item.txt\",\n",
    "    sep=\"\\|\",\n",
    "    names=[\n",
    "        'item_id', 'title', 'release_date', 'video_release_date', 'IMDb URL',\n",
    "        'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy',\n",
    "        'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "        'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n",
    "    ])\n",
    "df_movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-23T20:13:25.209Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: include_rts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Story (1995)\n"
     ]
    }
   ],
   "source": [
    "all_tweets = []\n",
    "for i, movie in df_movie.iterrows():\n",
    "    print(movie['title'])\n",
    "    \n",
    "    # Fetching tweets\n",
    "    tweets = fetch_tweets(movie['title'])\n",
    "    \n",
    "    # Converting raw response to JSON.\n",
    "    tweets = get_json_from_response(tweets)\n",
    "    \n",
    "    # Processing raw JSON, structured.    \n",
    "    processed_tweets = process_tweets(tweets,movie['title'], movie['item_id'])\n",
    "    \n",
    "    all_tweets.extend(processed_tweets)\n",
    "    time.sleep(45)\n",
    "df = dict_to_df(all_tweets)  \n",
    "df.to_csv(f'data/tweets.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-23T20:13:25.212Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Fetching tweets\n",
    "# tweets = fetch_tweets('Get Shorty (1995)')\n",
    "\n",
    "# # Converting raw response to JSON.\n",
    "# tweets = get_json_from_response(tweets)\n",
    "\n",
    "# # Processing raw JSON, structured.    \n",
    "# processed_tweets = process_tweets(tweets,'Get Shorty (1995)','1')\n",
    "\n",
    "# print(processed_tweets)\n",
    "# df = dict_to_df(processed_tweets)  \n",
    "# df.to_csv(f'data/twe1ets.csv', index=False)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "537.273px",
    "left": "36px",
    "top": "190.284px",
    "width": "261.775px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
